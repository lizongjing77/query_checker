{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from os import path\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-cloud-bigquery==1.24.0 six==1.14.0\n",
    "#!pip install --upgrade six>=1.13.0\n",
    "!pip uninstall google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: google-cloud-bigquery in c:\\anaconda\\lib\\site-packages (1.24.0)\n",
      "Requirement already up-to-date: six==1.14.0 in c:\\anaconda\\lib\\site-packages (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2.0dev,>=1.9.0 in c:\\anaconda\\lib\\site-packages (from google-cloud-bigquery) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0dev,>=1.15.0 in c:\\anaconda\\lib\\site-packages (from google-cloud-bigquery) (1.17.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in c:\\anaconda\\lib\\site-packages (from google-cloud-bigquery) (3.6.1)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.1.0 in c:\\anaconda\\lib\\site-packages (from google-cloud-bigquery) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<0.6dev,>=0.5.0 in c:\\anaconda\\lib\\site-packages (from google-cloud-bigquery) (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in c:\\anaconda\\lib\\site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery) (40.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in c:\\anaconda\\lib\\site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in c:\\anaconda\\lib\\site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in c:\\anaconda\\lib\\site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in c:\\anaconda\\lib\\site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in c:\\anaconda\\lib\\site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in c:\\anaconda\\lib\\site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery) (1.51.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in c:\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.9.0->google-cloud-bigquery) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery) (2019.3.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-cloud-bigquery six==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_path = \"/Users/li.zongjing/Desktop/浅野ツール/sample_query_v006.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQL_checker:\n",
    "    \n",
    "    def __init__(self,test_query_path=test_query_path):\n",
    "        if isinstance(test_query_path,str):\n",
    "            path = test_query_path\n",
    "        self.test_query_path = path\n",
    "        self.describe_list = ['PK' ,'MAX', 'MIN', 'AVG',\n",
    "                              'MODE', 'UNIQ_VAL', 'NULL_FRAC', \n",
    "                              'NULL_CNT']\n",
    "        \n",
    "        self.with_clause_string = self.read_sql()\n",
    "        self.table_loc = self.get_with_clauses()\n",
    "        self.table_list = list(self.get_with_clauses().keys())\n",
    "        self.clause_dict = self.with_clause()\n",
    "        self.table_dict = self.find_describe()\n",
    "        \n",
    "        for handler in logging.root.handlers[:]:\n",
    "            logging.root.removeHandler(handler)\n",
    "        logging.basicConfig(filename='qc_log.log',format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S',level=logging.DEBUG)\n",
    "        \n",
    "    #クエリを読み込む\n",
    "    def read_sql(self):\n",
    "        if path.exists(self.test_query_path):\n",
    "            logging.debug(\"sqlファイルを読み込み開始：\")\n",
    "            with open(self.test_query_path) as sql_txt:\n",
    "                copy = False    \n",
    "                with_clause_string = \"\"\" \"\"\"\n",
    "                for line in sql_txt:\n",
    "                    if line.strip() == \"/*with clauses start*/\":\n",
    "                        copy = True\n",
    "                        continue\n",
    "                    elif line.strip() == \"/*with clauses end*/\":\n",
    "                        copy = False\n",
    "                        continue\n",
    "                    elif copy:\n",
    "                        with_clause_string += line\n",
    "            logging.debug(\"読み込み完了\")          \n",
    "        else:\n",
    "            logging.debug('パスあるいはファイルは存在しない。')\n",
    "        return with_clause_string\n",
    "    \n",
    "    def read_whole_sql(self):\n",
    "        if path.exists(self.test_query_path):\n",
    "            with open(test_query_path) as sql_txt:\n",
    "                with_clause_string = \"\"\" \"\"\"\n",
    "                for line in sql_txt:\n",
    "                    with_clause_string += line\n",
    "        else:\n",
    "            logging.debug('パスあるいはファイルは存在しない。')\n",
    "        return with_clause_string\n",
    "    \n",
    "    #各with句のクエリを辞書型に収納\n",
    "    def with_clause(self):\n",
    "        logging.debug('with句クエリを抽出。')\n",
    "        clause_dict = {}\n",
    "        for table,loc in self.table_loc.items():\n",
    "            with_clause = self.read_whole_sql()[loc[0]:loc[1]]\n",
    "            clause_dict[table] = with_clause\n",
    "        logging.debug('with句クエリ抽出完了。')\n",
    "        return clause_dict  \n",
    "    \n",
    "    def convert_text(self,text, ran, to=None):\n",
    "        if to is None:\n",
    "            to = \" \" * (ran[1] - ran[0])\n",
    "        else:\n",
    "            assert len(to) == ran[1] - ran[0]\n",
    "        text_new = text[:ran[0]] + to + text[ran[1]:]\n",
    "        return text_new    \n",
    "    \n",
    "    def get_with_clauses(self):\n",
    "        logging.debug('with句テーブル名を抽出。')\n",
    "        with_clause_string_new = self.read_whole_sql()\n",
    "        \n",
    "        # /* */ 式のコメントを見つけて、空白に置換する\n",
    "        comment_all = list(re.finditer(r'/\\*[\\s\\S]*?\\*/', self.read_whole_sql()))\n",
    "        if len(comment_all) > 0:\n",
    "            for rr in comment_all:\n",
    "                with_clause_string_new = self.convert_text(with_clause_string_new, rr.span())    \n",
    "        \n",
    "        # そもそも with があるのか\n",
    "        res = re.findall(r'[Ww][Ii][Tt][Hh]', with_clause_string_new)\n",
    "        if len(res) == 0:\n",
    "            return None\n",
    "        \n",
    "        # WITH の終わりの \")\" を探す\n",
    "        # \")\" の後に \",\" が入らずに \"SELECT\" が来る\n",
    "        result = list(re.finditer(r'\\)[^,]*?[Ss][Ee][Ll][Ee][Cc][Tt]', with_clause_string_new))\n",
    "        with_end = result[0].span()[0]\n",
    "        \n",
    "        # 一番外側の \"()\" を探す\n",
    "        stack = []\n",
    "        with_clauses_pos = []  # with句のSELECT文の位置を格納\n",
    "        table_name_cand_last = []\n",
    "        for i, c in enumerate(with_clause_string_new):\n",
    "            if c == '(':\n",
    "                if i > with_end:\n",
    "                    # もう with から抜けている\n",
    "                    break\n",
    "                stack.append(i)\n",
    "            elif c == ')' and stack:\n",
    "                start = stack.pop()\n",
    "                if len(stack) == 0:\n",
    "                    # 一番外側\n",
    "                    with_clauses_pos.append((start+1, i))\n",
    "    \n",
    "        # \"()\" に入っていない部分\n",
    "        table_name_cand = [(0, with_clauses_pos[0][0])]\n",
    "        for j in range(len(with_clauses_pos)-1):\n",
    "            table_name_cand.append((with_clauses_pos[j][1], with_clauses_pos[j+1][0]))\n",
    "            \n",
    "        # テーブル名と位置を辞書で格納\n",
    "        with_clauses = OrderedDict()\n",
    "        for j in range(len(table_name_cand)):\n",
    "            serach_range = with_clause_string_new[table_name_cand[j][0]:table_name_cand[j][1]+1]\n",
    "            # AS の直前の文字列を取得\n",
    "            result = list(re.finditer(r'[Ss][Aa]\\s+\\S+?\\s', serach_range[::-1]))\n",
    "            result_span = result[0].span()\n",
    "            table_name = serach_range[-result_span[1]:-result_span[0]-3].replace(' ', '')\n",
    "            with_clauses[table_name] = with_clauses_pos[j]\n",
    "        logging.debug('with句テーブル名抽出完了。')\n",
    "        return with_clauses\n",
    "\n",
    "    #各with句で、ほしい処理とほしいカラムを辞書型に返す\n",
    "    #コメントが書いてあるテーブルを返す\n",
    "    def find_describe(self):\n",
    "        logging.debug('with句テーブルごとにカラムサマリを抽出。')\n",
    "        table_dict = {}\n",
    "        query_dict = {}\n",
    "        not_emp_table_dict = {}\n",
    "        for table,txt in self.clause_dict.items():\n",
    "            table_describe_dict = {}\n",
    "            for act in self.describe_list:\n",
    "                clause_list = re.findall(f'((.+?)/\\*QC_{act}\\*/)', txt)\n",
    "                for i in range(len(clause_list)):\n",
    "                    clause_list[i] = min(clause_list[i],key=len)    \n",
    "                root_list = []\n",
    "                for des in clause_list:\n",
    "                    des = des.replace(',', '',1).split(' ')[-1]\n",
    "                    if len(des.split(\".\")) != 1:\n",
    "                        des_split = des.split(\".\")[-1].split('/*')[0]\n",
    "                        root_list.append(des_split)\n",
    "                    else:\n",
    "                        root_list.append(des.split('/*')[0])\n",
    "                table_describe_dict[act] = root_list\n",
    "            table_dict[table] = table_describe_dict\n",
    "\n",
    "        for table in self.table_list:\n",
    "            if any(table_dict[table].values()):\n",
    "                not_emp_table_dict[table] = {}\n",
    "                for key,act in table_dict[table].items():\n",
    "                    if len(act) != 0:\n",
    "                        not_emp_table_dict[table][key] = act\n",
    "        logging.debug('with句テーブルごとにカラムサマリ抽出完了。')                \n",
    "        return not_emp_table_dict\n",
    "    \n",
    "    #チェックするためのクエリを生成する\n",
    "    def describe_query_writer(self):\n",
    "        logging.debug('with句テーブルごとにカラムサマリチェッククエリを生成。')\n",
    "        table_act_dict = {}\n",
    "        for table,dict in self.table_dict.items():\n",
    "            act_query = ''' '''\n",
    "            table_name = f'{table}'\n",
    "            from_clause = f'\\nFROM {table}'\n",
    "            non_pk_clause = f'SELECT\\n\\'{table_name}\\' AS table_name\\n, count(*) AS record_number\\n' \n",
    "            for key,col in dict.items():\n",
    "                if key == 'PK':\n",
    "                    case_when_clause = ''\n",
    "                    case_when_name = ''\n",
    "                    group_by_clause = ''\n",
    "                    for pk in col:\n",
    "                        case_when_clause += f', CASE WHEN {pk} is null THEN 1 ELSE 0 END AS {pk}_null_f\\n'\n",
    "                        case_when_name += f', max({pk}_null_f) as {pk}_null_f\\n'\n",
    "                        group_by_clause += f'{pk},'\n",
    "                    group_by_clause = group_by_clause[:-1]\n",
    "                    pk_clause = f'SELECT\\nmax(uni_f) as uni_f\\n{case_when_name}FROM\\n(\\nSELECT\\nCASE WHEN COUNT(*) > 1 THEN 1 ELSE 0 END AS uni_f\\n{case_when_clause}FROM\\n {table}\\n GROUP BY\\n {group_by_clause}\\n)'\n",
    "                else:\n",
    "                    if key == 'MAX':\n",
    "                        for c in col:\n",
    "                            act_query += f', max({c}) as {key}_{c}\\n'\n",
    "                    elif key == 'MIN':\n",
    "                        for c in col:\n",
    "                            act_query += f', min({c}) as {key}_{c}\\n'\n",
    "                    elif key == 'AVG':\n",
    "                        for c in col:\n",
    "                            act_query += f', avg({c}) as {key}_{c}\\n'\n",
    "                    elif key == 'MODE':\n",
    "                        #for c in col:\n",
    "                        #    act_query += f', (SELECT {c} FROM {table} GROUP BY {c} HAVING COUNT(*)>=all((SELECT COUNT(*) FROM {table} GROUP BY {c})) limit 1) as {key}_{c}\\n'\n",
    "                        pass\n",
    "                    elif key == 'UNIQ_VAL':\n",
    "                        for c in col:\n",
    "                            act_query += f', count(distinct({c})) as {key}_{c}\\n'\n",
    "                    elif key == 'NULL_FRAC':\n",
    "                        for c in col:\n",
    "                            act_query += f', 1 - (count({c})/count(*)) as {key}_{c}\\n'  \n",
    "                    elif key == 'NULL_CNT':\n",
    "                        for c in col:\n",
    "                            act_query += f', count(*) - count({c}) as {key}_{c}\\n'\n",
    "            non_pk_clause += act_query + from_clause\n",
    "            select_clause = f'SELECT\\n*\\nFROM\\n({non_pk_clause}) as npk\\nleft outer join\\n({pk_clause}) as pk\\non 1=1'\n",
    "            table_act_dict[table] = self.with_clause_string + select_clause\n",
    "        logging.debug('with句テーブルごとにカラムサマリクエリ生成完了。')    \n",
    "        return table_act_dict\n",
    "    \n",
    "    #sqlファイルを出力\n",
    "    def write_sql_file(self):\n",
    "        logging.debug('with句テーブルごとのチェッククエリをファイルに作成。')\n",
    "        i = 0\n",
    "        for table,query in self.describe_query_writer().items():\n",
    "            i += 1\n",
    "            with open(f'summary_cheker_{i}_{table}.sql','w') as file:\n",
    "                file.write(query)\n",
    "        logging.debug('with句テーブルごとのチェッククエリをファイルに作成完了。')\n",
    "        return self\n",
    "    \n",
    "    #DBサーバーから帰ってくる結果を認識する\n",
    "    def install_bg(self):\n",
    "        \n",
    "        logging.debug('以下のコマンドを使って、big query接続用のライブラリーをインストール。')\n",
    "        print('!pip install --upgrade google-cloud-bigquery')\n",
    "        print('from google.cloud import bigquery')\n",
    "        print('from google.oauth2 import service_account')\n",
    "        \n",
    "    def connect_bg(self,path='path/to/file.json',project_id='my-bq'):\n",
    "    \n",
    "        logging.debug('big queryに接続。')\n",
    "        credentials = service_account.Credentials.from_service_account_file(\n",
    "        path)\n",
    "        project_id = project_id\n",
    "        client = bigquery.Client(credentials=credentials,project=project_id)\n",
    "        logging.debug('big queryに接続成功。')\n",
    "    \n",
    "        return client\n",
    "    \n",
    "    def run_query(self,mode='standard',path='path/to/file.json',project_id='my-bq'):\n",
    "        results = {}\n",
    "        client = self.connect_bg(path=path,project_id=project_id)\n",
    "        for table,query in self.describe_query_writer().items():\n",
    "            logging.debug(f'{table} 実行中。')\n",
    "            if mode == 'standard':\n",
    "                query_job = client.query(query).to_dataframe()\n",
    "            elif mode == 'legacy':\n",
    "                job_config.use_legacy_sql = True\n",
    "                query_job = client.query(query, job_config=job_config).to_dataframe()\n",
    "            \n",
    "            results[table] = results\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=SQL_checker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_clause_string = test.with_clause_string\n",
    "table_list = test.table_list\n",
    "with_clause = test.clause_dict\n",
    "table_dict = test.table_dict\n",
    "describe_list = test.describe_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['temp_prod_mst', 'temp_log', 'datamart', 'tough_clause']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temp_prod_mst': {'PK': ['shop_id', 'prod_id']},\n",
       " 'temp_log': {'PK': ['uu', 'ss'], 'MAX': ['page_num']},\n",
       " 'datamart': {'MAX': ['uu']},\n",
       " 'tough_clause': {'PK': ['yy', 'ii', 'first_date'],\n",
       "  'MAX': ['first_date', 'max_p', 'whatif', 'pppp'],\n",
       "  'MIN': ['pp', 'pppp'],\n",
       "  'AVG': ['whatif'],\n",
       "  'MODE': ['pppp'],\n",
       "  'UNIQ_VAL': ['t'],\n",
       "  'NULL_FRAC': ['tt'],\n",
       "  'NULL_CNT': ['ttt', 'tttt']}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SQL_checker at 0x21712d60128>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.write_sql_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/file.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b6cba990ddb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'standard'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'path/to/file.json'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'my-bq'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-c6b56eb32975>\u001b[0m in \u001b[0;36mrun_query\u001b[1;34m(self, mode, path, project_id)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'standard'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'path/to/file.json'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'my-bq'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect_bg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquery\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe_query_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{table} 実行中。'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-c6b56eb32975>\u001b[0m in \u001b[0;36mconnect_bg\u001b[1;34m(self, path, project_id)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'big queryに接続。'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         credentials = service_account.Credentials.from_service_account_file(\n\u001b[1;32m--> 229\u001b[1;33m         path)\n\u001b[0m\u001b[0;32m    230\u001b[0m         \u001b[0mproject_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproject_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\google\\oauth2\\service_account.py\u001b[0m in \u001b[0;36mfrom_service_account_file\u001b[1;34m(cls, filename, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \"\"\"\n\u001b[0;32m    217\u001b[0m         info, signer = _service_account_info.from_filename(\n\u001b[1;32m--> 218\u001b[1;33m             \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"client_email\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"token_uri\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m         )\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_signer_and_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\google\\auth\\_service_account_info.py\u001b[0m in \u001b[0;36mfrom_filename\u001b[1;34m(filename, require)\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0ma\u001b[0m \u001b[0msigner\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequire\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/file.json'"
     ]
    }
   ],
   "source": [
    "test.run_query(mode='standard',path='path/to/file.json',project_id='my-bq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
